<!-- # Autonomous Paper Ball Catcher

## Overview
The **Autonomous Paper Ball Catcher** is a mobile robotics project that integrates **multi-camera vision**, **object tracking**, and **robot motion planning**.  
The goal is for a mobile dustbin robot to autonomously catch a thrown paper ball by predicting its landing point using multiple stationary cameras.

Unlike conventional approaches that rely on an onboard camera, this system uses **environment-mounted RGB cameras** that observe the entire arena to track both the ball and the robot in a shared world coordinate frame.

---

## Current Status
- âœ… Robot hardware (omnidirectional mecanum base) assembled.  
- ðŸ•“ Motion control testing pending.  
- ðŸ› ï¸ Camera-based perception and trajectory estimation not started yet.  
- âš™ï¸ ROS 2 + OpenCV setup and calibration tools under preparation.

---

## Planned Modules
| Module | Description | Status |
|--------|--------------|--------|
| **Robot Motion** | Control of mecanum drive via ESP32 | âš™ï¸ In Progress |
| **Multi-Camera Setup** | Calibration + synchronization | â³ Pending |
| **Perception** | Ball + robot detection and 3D triangulation | â³ Pending |
| **Trajectory Estimation** | EKF-based 3D tracking under gravity | â³ Pending |
| **Planning & Control** | Path planning to intercept predicted landing point | â³ Pending |

---

## Hardware Stack
- **Robot Base:** Custom omnidirectional mecanum platform  
- **Motor Drivers:** TB6612FNG  
- **Controller:** ESP32 (motor control and communication)  
- **Cameras:** Multiple stationary RGB webcams (Kreo Owl Full HD 60 FPS planned)  
- **Processing:** Laptop running ROS 2 and OpenCV  
- **Power:** 12 V Li-ion 3S battery  
- **Hub:** Powered USB 3.0 hub with 5 V 2â€“3 A adapter

---

## Directory Structure
```

hardware/     â†’ Robot electronics, motor control
vision/       â†’ Camera calibration, detection, triangulation
control/      â†’ Motion planner, robot control algorithms
ros2_ws/      â†’ ROS 2 workspace for camera + robot integration
data/         â†’ Images, videos, and calibration logs
docs/         â†’ Reports, figures, and project documentation

````

---

## Getting Started
```bash
# Clone repository
git clone https://github.com/Zia-Mahmood/Autonomous-Paper-Ball-Catcher.git
cd Autonomous-Paper-Ball-Catcher

# (Future) Install dependencies
pip install -r requirements.txt
```` -->

# **Autonomous Paper Ball Catcher**

### *A Multi-Camera System for Real-Time 3D Ball Tracking & Robotic Interception*

Team **Slam-Dunk** â€” *Zia Mahmood Hussain & Nikhil Singh*

---

# **1. Overview**

This project implements a **real-time multi-camera perception and prediction pipeline** that detects a thrown paper ball, reconstructs its 3D trajectory, predicts its future path, and commands a mobile robot (mecanum drive) to move to the interception point.

The system achieves:

* **60 FPS ball detection**
* **40â€“45 FPS triangulation**
* **Accurate 3D reconstruction of the ball and robot**
* **Fast, low-latency prediction using RLS (Recursive Least Squares)**
* **Open3D-based interception simulation**
* **Full end-to-end pipeline working in real time**

**Current limitation:**
Projectile-motion prediction is **not yet perfectly refined** â€” it performs excellently for **free-fall / slow arcs**, but accuracy drops for **fast, angled projectile throws**. This is the next target for improvement.

Everything else works end-to-end.

---

# **2. System Pipeline**

```
Multi-Camera Capture (60 FPS)
        â†“
HSV Ball Detection + AprilTag Robot Pose
        â†“
Stereo Triangulation (40â€“45 FPS)
        â†“
Trajectory Estimation (RLS / EKF / LKF)
        â†“
Intercept Prediction (landing point + intercept time)
        â†“
Robot Controller (real robot + simulation)
```

---

# **3. Features**

### **âœ” Multi-Camera Calibration**

* Intrinsics & distortion correction
* World-frame alignment using AprilTags
* Automatic exposure/gain tuning for reliable detection

### **âœ” AprilTag Robot Localization**

* Stable 6-DoF robot pose in the shared world frame
* Used for intercept planning

### **âœ” High-Speed Ball Detection**

* HSV thresholding
* Contour extraction
* Noise filtering
* 60 FPS sustained across both cameras

### **âœ” Stereo Triangulation**

* Epipolar gating for view association
* `triangulatePoints()`
* 3D output at ~40â€“45 FPS
* Good depth stability for practical throwing ranges

### **âœ” Prediction Models**

Implemented models:

* Linear Kalman Filter
* Extended Kalman Filter
* Sliding Window Regression
* **Recursive Least Squares (RLS) â€” best performing overall**

RLS gives:

* Smooth trajectories
* Very low RMSE for free-fall
* Strong real-time performance

### **âœ” Intercept Planner**

* Computes whether the robot can reach the intercept point in time
* Uses velocity + kinematic constraints
* Supports both simulation and real control

### **âœ” Open3D Simulation**

* Real-time 3D playback
* Two modes:

  1. **Full trace mode** (past + future trajectory)
  2. **Prediction-only mode** (future trajectory only)

---

# **4. Project Structure**

```
.
â”œâ”€â”€ vision/
â”‚   â”œâ”€â”€ calibration/          # Intrinsics, extrinsics
â”‚   â”œâ”€â”€ config/               # Auto tuning camera lighting settigns
â”‚   â”œâ”€â”€ detection/            # Ball + AprilTag detection
â”‚   â”œâ”€â”€ triangulation/        # Multi-camera 3D reconstruction
â”‚   â”œâ”€â”€ predictor/            # RLS, EKF, LKF trajectory models
â”‚   â”œâ”€â”€ planner/              # Intercept solver
â”‚   â”œâ”€â”€ publisher/            # Publishes raw images using zmq + stack datastructure
â”‚   â””â”€â”€ visualization/        # Open3D + plotting tools
â”‚
â”œâ”€â”€ hardware/
â”‚   â”œâ”€â”€ Motion_scripts/ 
â”‚   â”œâ”€â”€ schematics/            
â”‚   â””â”€â”€ test_motion/          
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ april_tags/
â”‚   â”œâ”€â”€ ball_detection_logs/
â”‚   â”œâ”€â”€ triangulation_logs/
â”‚   â”œâ”€â”€ prediction_logs/
â”‚   â””â”€â”€ simulation/
â”‚
â”œâ”€â”€ results/                  # images/GIFs
â””â”€â”€ README.md
```

---

# **5. How to Run Everything**

## **5.1 Calibration**

```
python vision/calibration/calibrate_camera.py
```

Outputs:

```
camera_calibration_kreo1.npz
camera_calibration_kreo2.npz
camera_calibration_mobile.npz
```

---

## **5.2 Run Detection**

### **AprilTags**

```
python vision/detection/detect_apriltags.py
```

### **Ball Detection**

```
python vision/detection/detect_ball.py
```

Outputs detection overlays and logs.

---

## **5.3 Triangulation**

```
python vision/triangulation/triangulation.py
```

Produces 3D points at ~45 FPS.

---

## **5.4 Prediction (RLS recommended)**

```
python vision/predictor/predictor.py
```

Outputs:

* Predicted trajectory
* Landing/intercept point
* RMSE logs

---

## **5.5 Interception Planner**

```
python vision/planner/planner.py
```

Inputs:

* predicted bivariate polynomial from RLS
* robot max speed
* current robot pose

Outputs:

* intercept point
* can-catch / cannot-catch flag

---

## **5.6 3D Simulation (Open3D)**

### **Full trace mode:**

```
python vision/visualization/visualize_with_trace.py
```

### **Prediction-only mode:**

```
python vision/visualization/visualize_without_trace.py
```

Produces 3D visualization of:

* ball trajectory
* predicted trajectory
* robot motion

---

# **6. Performance Summary**

### **FPS**

| Stage              | FPS   |
| ------------------ | ----- |
| Capture            | 60    |
| Ball Detection     | 60    |
| AprilTag Detection | 40    |
| Triangulation      | 38â€“45 |
| RLS Predictor      | 30â€“40 |

### **Prediction Performance**

* **Free-fall:** extremely accurate
* **Slow arcs:** accurate
* **Fast projectile throws:** works, but not yet refined â†’ next improvement target

---

# **7. Media Placeholders**

<!-- ### **7.1 Ball Detection**

```
results/detection_1.png
results/detection_2.png
```

### **7.2 Triangulation Frames**

```
results/triangulation_view_1.png
results/triangulation_view_2.png
results/triangulation_3d_plot.png
```

### **7.3 Prediction Visualizations**

```
results/rls_fit_front_view.png
results/rls_fit_side_view.png
results/rls_fit_top_view.png
results/prediction_error_curve.png
```

### **7.4 Simulation Videos**

```
results/sim_full_trace.mp4
results/sim_predicted_only.mp4
```

### **7.5 Real-World Demonstration GIFs**

```
results/real_detection.gif
results/real_triangulation.gif
results/real_robot_intercept.gif
``` -->

### Google Drive Link 

[Google Drive Link to Result Videos and Images](https://drive.google.com/drive/folders/1lEcQoplVQeGwwzSsM_GPhqJEsyI0ENAF?usp=drive_link)

---

# **8. Future Work**

### ðŸ”§ **1. Improve projectile-motion prediction**

Current predictor is excellent for free-fall and short arcs, but for high-speed projectile throws:

* Early-trajectory noise magnifies velocity estimation error
* Drag affects trajectories in nontrivial ways
* Some throws require more sophisticated modeling

**Planned fixes:**

* Add drag model or hybrid physics + RLS
* Use multi-frame batch fitting (better initial velocity estimate)
* Smooth 2D detections temporally before triangulation
* Optional: migrate predictor to C++ for higher FPS

### ðŸš€ **2. Multi-camera fusion beyond stereo**

Adding a **third camera** improves depth accuracy dramatically.

### ðŸ¤– **3. Full end-to-end real-time interception**

The perception â†’ planner â†’ robot control loop is ready for tighter integration and full-speed live demos.

---

# **9. Authors**

* **Zia Mahmood Hussain** 
* **Nikhil Singh** 
